# cython: wraparound=False, nonecheck=False, boundscheck=False, cdivision=True
# cython: language_level=3, binding=True, embedsignature=True, annotation_typing=True
# distutils: language = c++
# distutils: extra_compile_args = -std=c++14 -O3 -march=native -DNDEBUG
# distutils: extra_link_args = -std=c++14

"""
Annoy Library Cython Implementation - Template Generator.

This file uses NumPy-style Tempita templating to generate all type combinations
for Annoy indices, eliminating code repetition and enabling centralized management.

ALL INDEX METHODS with C-Level Method Dispatch:

- add_item, build, save, load, unload
- get_nns_by_item, get_nns_by_vector
- get_item, get_distance
- get_n_items, get_n_trees, get_f
- unbuild, on_disk_build
- serialize, deserialize
- set_seed, verbose

Uses static C function pointer tables for type-safe, zero-overhead dispatch.

Based on:
- annoymodule.cc (reference implementation)
- NumPy's _bounded_integers.pyx.in pattern
- Modern sklearn serialization patterns
"""

# ==============================================================================
# Python Imports
# ==============================================================================

import warnings
import threading
import json
import pickle
import tempfile
import os
from pathlib import Path
from typing import Optional, Union, List, Dict, Any, Tuple

import numpy as np
cimport numpy as cnp

# NumPy C API initialization
cnp.import_array()

# ==============================================================================
# Cython Imports
# ==============================================================================

from libc.stdlib cimport malloc, free
from libc.string cimport memcpy, memset
from libc.stdint cimport int32_t, int64_t, uint32_t, uint64_t, uint8_t
from libcpp cimport bool as cbool
from libcpp.vector cimport vector
from libcpp.vector cimport vector as cpp_vector
from libcpp.string cimport string as cpp_string

# ==============================================================================
# Constants Configuration
# ==============================================================================

include "annoylib.pxi"

# ==============================================================================
# Template Configuration - Single Source of Truth
# ==============================================================================
{{
py:
# -----------------------------------------------------------------------------
# Type System Configuration (same as pxd.in)
# -----------------------------------------------------------------------------

# Index size types (S = int32_t or int64_t)
index_types = [
    {'name': 'int32', 'ctype': 'int32_t', 'cython': 'int32_t', 'numpy': 'int32', 'bits': 32, 'enum': 'INDEX_INT32', 'max_items': 2147483647},
    {'name': 'int64', 'ctype': 'int64_t', 'cython': 'int64_t', 'numpy': 'int64', 'bits': 64, 'enum': 'INDEX_INT64', 'max_items': 9223372036854775807},
]

# Data precision types (T = float, double, or unsigned for Hamming)
data_types = [
    # cpp
    {'name': 'float', 'ctype': 'float', 'cython': 'float', 'numpy': 'float32', 'bits': 32, 'enum': 'DATA_FLOAT', 'precision': 'single'},
    {'name': 'double', 'ctype': 'double', 'cython': 'double', 'numpy': 'float64', 'bits': 64, 'enum': 'DATA_DOUBLE', 'precision': 'double'},
    # canonical
    {'name': 'float32', 'ctype': 'float', 'cython': 'float', 'numpy': 'float32', 'bits': 32, 'enum': 'DATA_FLOAT32', 'precision': 'single'},
    {'name': 'float64', 'ctype': 'double', 'cython': 'double', 'numpy': 'float64', 'bits': 64, 'enum': 'DATA_FLOAT64', 'precision': 'double'},
    # hamming
    {'name': 'uint32', 'ctype': 'uint32_t', 'cython': 'uint32_t', 'numpy': 'uint32', 'bits': 32, 'enum': 'DATA_UINT32', 'precision': 'binary', 'unsigned': True},
    {'name': 'uint64', 'ctype': 'uint64_t', 'cython': 'uint64_t', 'numpy': 'uint64', 'bits': 64, 'enum': 'DATA_UINT64', 'precision': 'binary', 'unsigned': True},
]

# Distance metrics
metrics = [
    {
        'name': 'Angular',
        'enum': 'METRIC_ANGULAR',
        'value': 1,
        'aliases': ['angular', 'cosine'],
        'description': 'Cosine similarity (normalized dot product)',
        'data_types': ['float32', 'float64'],
        'default_data': 'float',
    },
    {
        'name': 'Euclidean',
        'enum': 'METRIC_EUCLIDEAN',
        'value': 2,
        'aliases': ['euclidean', 'l2', 'lstsq'],
        'description': 'L2 distance (Euclidean norm)',
        'data_types': ['float32', 'float64'],
        'default_data': 'float',
    },
    {
        'name': 'Manhattan',
        'enum': 'METRIC_MANHATTAN',
        'value': 3,
        'aliases': ['manhattan', 'l1', 'cityblock', 'taxicab'],
        'description': 'L1 distance (Manhattan norm)',
        'data_types': ['float32', 'float64'],
        'default_data': 'float',
    },
    {
        'name': 'DotProduct',
        'enum': 'METRIC_DOTPRODUCT',
        'value': 4,
        'aliases': ['dot', 'dotproduct', 'inner', 'innerproduct', '@', '.'],
        'description': 'Negative dot product (maximum inner product search)',
        'data_types': ['float32', 'float64'],
        'default_data': 'float',
    },
    {
        'name': 'Hamming',
        'enum': 'METRIC_HAMMING',
        'value': 5,
        'aliases': ['hamming'],
        'description': 'Hamming distance (bit differences)',
        'data_types': ['uint32', 'uint64'],  # Internal representation
        'default_data': 'uint32',

        'needs_wrapper': True,
        'external_interface': 'float',  # External API uses float
    },
]

# Generate all valid combinations
def generate_type_combinations(metrics, index_types, data_types):
    """
    Generate all valid (metric, index_type, data_type) combinations.

    Returns list of dicts with keys:
    - metric, metric_enum, metric_lower
    - index_type, index_ctype
    - data_type, data_ctype
    - class_name, cpp_template, description
    """
    combinations = []

    for metric in metrics:
        for index_type in index_types:
            for data_type in data_types:
                if data_type['name'] not in metric['data_types']:
                    continue

                # Create combination descriptor
                combo = {
                    'metric': metric['name'],
                    'metric_enum': metric['enum'],
                    'metric_lower': metric['name'].lower(),
                    'index_type': index_type['name'],
                    'index_enum': index_type['enum'],
                    'index_ctype': index_type['ctype'],
                    'index_numpy_type': index_type['numpy'],
                    'data_type': data_type['name'],
                    'data_enum': data_type['enum'],
                    'data_ctype': data_type['ctype'],
                    'data_numpy_type': data_type['numpy'],
                    'class_name': (
                        f"CAnnoyIndex{metric['name']}"
                        f"{index_type['name'].capitalize()}"
                        f"{data_type['name'].capitalize()}"
                    ),
                    'factory_name': f"_create_{metric['name'].lower()}_{index_type['name']}_{data_type['name']}",
                    'cpp_template': (
                        (
                            f"Annoy::AnnoyIndex<"
                            if {metric['name']} == ""
                            else f"Annoy::AnnoyIndex<"
                        ),
                        f"{index_type['ctype']}, "
                        f"{data_type['ctype']}, "
                        f"Annoy::{metric['name']}, "
                        f"Annoy::Kiss64Random, "
                        f"Annoy::AnnoyIndexSingleThreadedBuildPolicy>"
                    ),
                    'description': (
                        f"{metric['description']} with "
                        f"{index_type['bits']}-bit indices and "
                        f"{data_type['bits']}-bit "
                        f"{data_type.get('precision', 'float')} data"
                    ),
                    # Hamming uses uint internally but presents float interface
                    'needs_wrapper': metric.get('needs_wrapper', False),
                    'external_interface': metric.get('external_interface', 'float'),
                }
                combinations.append(combo)

    return combinations

# Generate all combinations
type_combinations = generate_type_combinations(
    metrics=metrics,
    index_types=index_types,
    data_types=data_types,
)

# Metric alias mapping for parsing
metric_aliases = {}
for metric in metrics:
    for alias in metric['aliases']:
        metric_aliases[alias] = metric['enum']
}}

# Import all declarations from annoylib.pxd
from scikitplot.annoy._annoy.annoylib cimport (
    # Enums
    MetricType, IndexType, DataType,
    METRIC_UNKNOWN, METRIC_ANGULAR, METRIC_EUCLIDEAN, METRIC_MANHATTAN,
    METRIC_DOTPRODUCT, METRIC_HAMMING,
    INDEX_INT32, INDEX_INT64,
    DATA_FLOAT, DATA_FLOAT32, DATA_DOUBLE, DATA_FLOAT64, DATA_UINT32, DATA_UINT64,

    # C-level dispatch
    ClassFactoryEntryLock,
    MethodDispatchEntryLock,
    _lookup_class_factory,
    _lookup_method_factory,
    create_index_from_config,
    init_dispatch_exact,

    # C++ classes (will be generated from template)
    CAnnoyIndex,
    {{for combo in type_combinations}}
    {{combo['class_name']}},
    {{endfor}}
)

# ==============================================================================
# Module Metadata
# ==============================================================================

__version__ = "1.17.3"
__author__ = "Annoy Contributors"
__module__ = "scikitplot.annoy._annoy"

SERIALIZATION_VERSION = 1

# ==============================================================================
# Exception Classes
# ==============================================================================

class AnnoyException(Exception):
    """Base exception for Annoy errors."""
    pass

class AnnoyIndexException(AnnoyException):
    """Exception raised for index operation errors."""
    pass

class AnnoyNotBuiltException(AnnoyIndexException):
    """Exception raised when querying before building."""
    pass

class AnnoyDimensionException(AnnoyIndexException):
    """Exception raised for dimension mismatches."""
    pass

class AnnoyMetricException(AnnoyIndexException):
    """Exception raised for invalid metrics."""
    pass

class AnnoyMemoryException(AnnoyIndexException):
    """Exception raised for memory allocation errors."""
    pass

class AnnoySerializationException(AnnoyException):
    """Exception raised for serialization errors."""
    pass

# -----------------------------------------------------------------------------
# IndexType Helper
# -----------------------------------------------------------------------------

def _parse_index_dtype(index_dtype: Optional[Union[str, IndexType]]) -> IndexType:
    """
    Parse index dtype input into an IndexType enum.

    Parameters
    ----------
    index_dtype : str | IndexType | None

    Returns
    -------
    IndexType

    Raises
    ------
    TypeError
        If index_dtype type is invalid.
    ValueError
        If index_dtype value is unknown.
    """
    if index_dtype is None:
        return INDEX_INT32

    if isinstance(index_dtype, IndexType):
        return index_dtype

    if not isinstance(index_dtype, str):
        raise TypeError(
            "index_dtype must be str, IndexType, or None "
            f"(got {type(index_dtype).__name__})"
        )

    dtype_str = str(index_dtype).lower().strip()
    if not dtype_str:
        raise ValueError("index_dtype must be a non-empty string")

    try:
        # if dtype_str in _str2indextype_map:
        return _str2indextype_map[dtype_str]
    except KeyError as e:
        valid = ", ".join(sorted(_str2indextype_map))
        raise ValueError(
            f"Unknown index_dtype '{index_dtype}'. Valid index dtypes: {valid}"
        ) from e

# -----------------------------------------------------------------------------
# DataType Helper
# -----------------------------------------------------------------------------

def _parse_dtype(dtype: Optional[Union[str, DataType]]) -> DataType:
    """
    Parse dtype input into a DataType enum.

    Parameters
    ----------
    dtype : str | DataType | None

    Returns
    -------
    DataType

    Raises
    ------
    TypeError
        If dtype type is invalid.
    ValueError
        If dtype value is unknown.
    """
    if dtype is None:
        return DATA_FLOAT32

    if isinstance(dtype, DataType):
        return dtype

    if not isinstance(dtype, str):
        raise TypeError(
            "dtype must be str, DataType, or None "
            f"(got {type(dtype).__name__})"
        )

    dtype_str = str(dtype).lower().strip()
    if not dtype_str:
        raise ValueError("dtype must be a non-empty string")

    try:
        # if dtype_str in _str2datatype_map:
        return _str2datatype_map[dtype_str]
    except KeyError as e:
        valid = ", ".join(sorted(_str2datatype_map))
        raise ValueError(
            f"Unknown dtype '{dtype}'. Valid dtypes: {valid}"
        ) from e

# -----------------------------------------------------------------------------
# MetricType Helper
# -----------------------------------------------------------------------------

def _parse_metric(metric: Optional[Union[str, MetricType]]) -> MetricType:
    """
    Parse metric input into a MetricType enum.

    Parameters
    ----------
    metric : str | MetricType | None
        Metric identifier. Strings are case-insensitive.

    Returns
    -------
    MetricType
        Parsed metric enum.

    Raises
    ------
    TypeError
        If metric type is invalid.
    ValueError
        If metric value is unknown.
    """
    if metric is None:
        return METRIC_ANGULAR

    if isinstance(metric, MetricType):
        return metric

    # Strict string handling (no implicit coercion)
    if not isinstance(metric, str):
        raise TypeError(
            "metric must be str, MetricType, or None "
            f"(got {type(metric).__name__})"
        )
    metric_lower = str(metric).lower().strip()

    if not metric_lower:
        raise ValueError("metric must be a non-empty string")

    try:
        # if metric_lower in _str2metrictype_map:
        return _str2metrictype_map[metric_lower]
    except KeyError as e:
        valid = ", ".join(sorted(_str2metrictype_map))
        raise AnnoyMetricException(
            f"Unknown metric '{metric}'. Valid metrics: {valid}"
        ) from e

# -----------------------------------------------------------------------------
# Validation Helper
# -----------------------------------------------------------------------------

# def _validate_type_combination(
#     MetricType metric,
#     IndexType index_type,
#     DataType data_type
# ) -> cbool:
#     """
#     Validate metric/index/data type combination.
#
#     Returns
#     -------
#     bool
#         True if combination is valid.
#
#     Raises
#     ------
#     ValueError
#         If combination is invalid.
#     """
#     # Hamming requires uint data types internally
#     # But accepts float at the API level (wrapper handles conversion)
#     if metric == METRIC_HAMMING:
#         # At API level, user passes float, wrapper converts to uint
#         # So we don't restrict here
#         pass
#     elif data_type == DATA_UINT32 or data_type == DATA_UINT64:
#         raise ValueError(
#             "uint data types are only valid for Hamming metric. "
#             "Use float or double for other metrics."
#         )
#
#     return True

# Continue in Part 2...
# Part 2: Main AnnoyIndex Class with ALL Methods

# ==============================================================================
# Main AnnoyIndex Class - User-Facing API
# ==============================================================================

cdef class AnnoyIndex:
    """
    Approximate Nearest Neighbor index with runtime type selection.

    Complete implementation with all index methods using C-level dispatch.

    This class provides a unified interface to Annoy's templated C++ index
    classes, with lazy initialization and flexible dtype/index_dtype selection.

    Follows modern sklearn patterns with serialization, context management,
    and estimator API support.

    Parameters
    ----------
    f : int or None, default=None
        Vector dimension. If None, inferred from first vector.
    metric : str or None, default=None
        Distance metric: 'angular', 'euclidean', 'manhattan', 'dot', 'hamming'.
        If None and f > 0, defaults to 'angular' with FutureWarning.
    n_neighbors : int, default=5
        Number of neighbors for sklearn-style transform().
    seed : int or None, default=None
        Random seed for reproducible builds.
    verbose : bool, default=False
        Enable verbose logging.
    index_dtype : str or None, default='int32'
        Index type: 'int32' or 'int64'.
    dtype : str or None, default='float'
        Data type: 'float', 'float32', 'double', 'float64'.
        Hamming accepts float/double (converted to uint internally).
    **kwargs
        Additional parameters (on_disk_path, prefault, etc.).

    Attributes
    ----------
    f : int
        Vector dimension.
    metric : str
        Distance metric ('angular', 'euclidean', 'manhattan', 'dot', 'hamming').
    n_neighbors : int
        Number of neighbors for transform().
    n_features_in_ : int
        Number of features seen during fit (sklearn compat).
    n_features_out_ : int
        Number of output features (equals n_neighbors).
    feature_names_in_ : tuple or None
        Feature names if provided during fit.
    y : list or None
        Dense labels (sklearn compat).
    y_map : dict or None
        Sparse label mapping {item_id: label}.

    Examples
    --------
    >>> # Basic usage
    >>> index = AnnoyIndex(128, metric='angular')
    >>> for i in range(1000):
    ...     vec = np.random.randn(128).astype('float32')
    ...     index.add_item(i, vec)
    >>> index.build(10)
    >>> neighbors = index.get_nns_by_item(0, 10)

    >>> # With explicit dtype
    >>> index = AnnoyIndex(128, metric='dot', dtype='double', index_dtype='int64')

    >>> # Sklearn-style usage
    >>> index = AnnoyIndex(n_neighbors=5)
    >>> index.fit(X)
    >>> neighbors = index.transform(X_query)

    >>> # Context manager
    >>> with AnnoyIndex(128, metric='angular') as index:
    ...     index.add_item(0, vec)
    ...     index.build(10)

    >>> # Serialization
    >>> state = index.get_state()
    >>> index2 = AnnoyIndex()
    >>> index2.set_state(state)
    """

    # Class attribute
    __module__ = "scikitplot.annoy._annoy"

    # C-level attributes (mirrors annoymodule.cc py_annoy struct)
    cdef MetricType _metric_type
    cdef IndexType _index_type
    cdef DataType _data_type

    cdef ClassFactoryEntryLock* _class  # Cached annoy index class
    cdef MethodDispatchEntryLock* _method_dispatch  # Cached wrapper method dispatch
    cdef CAnnoyIndex* _ptr  # Generic C++ index pointer

    # State flags
    cdef cbool _index_created
    cdef cbool _index_built

    # Python attributes (public for sklearn compat)
    cdef public int f
    cdef public str metric
    cdef public str dtype
    cdef public str index_dtype
    cdef public size_t n_neighbors
    # Additional attributes
    cdef public cbool prefault
    cdef public int schema_version
    cdef public object on_disk_path

    # Pending configuration (applied on lazy init)
    cdef cbool _has_pending_seed
    cdef uint64_t _pending_seed
    cdef cbool _has_pending_verbose
    cdef cbool _pending_verbose

    # Sklearn compatibility
    cdef public object y  # Dense labels
    cdef public object y_map  # Sparse label mapping {item_id -> label}
    cdef public object feature_names_in_  # SLEP007

    # Thread safety
    cdef public object lock

    cdef class AnnoyIndexWrapper:
        cdef MethodDispatchEntryLock _dispatch
        cdef bint _dispatch_initialized

        def __cinit__(self, metric, index_type, data_type):
            init_dispatch_exact(
                &self._dispatch,
                metric,
                index_type,
                data_type
            )
            self._dispatch_initialized = True

    def __cinit__(self):
        """C-level initialization."""
        # Type configuration
        self._metric_type = METRIC_UNKNOWN
        self._index_type = INDEX_INT32
        self._data_type = DATA_FLOAT32

        # C-level pointers - MUST ALL BE INITIALIZED TO NULL!
        self._class = NULL            # annoy_index
        self._method_dispatch = NULL  # method wrapper
        self._ptr = NULL              # initialized annoy_index

        # State flags
        self._index_created = False
        self._index_built = False

        # Python attributes
        self.f = 0
        self.metric = None
        self.dtype = None
        self.index_dtype = None
        self.n_neighbors = DEFAULT_N_NEIGHBORS
        # Additional attributes
        self.prefault = DEFAULT_PREFAULT
        self.schema_version = DEFAULT_SCHEMA_VERSION
        self.on_disk_path = None

        # Pending configuration
        self._has_pending_seed = False
        self._pending_seed = 0
        self._has_pending_verbose = False
        self._pending_verbose = False

        # Sklearn compatibility
        self.y = None
        self.y_map = None
        self.feature_names_in_ = None

    def __dealloc__(self):
        """C-level cleanup."""
        # if self._ptr != NULL:
        self._free_index()

    def __init__(
        self,
        f=None,
        metric=None,
        *,
        n_neighbors=5,
        seed=None,
        verbose=False,
        prefault=False,
        schema_version=1,
        on_disk_path=None,
        index_dtype=None,
        dtype=None,
        **kwargs
    ):
        """
        Initialize Annoy index with lazy construction.

        The C++ index is NOT created immediately. It's created lazily
        on first use (add_item, build, load, etc.).
        """
        # Parse and validate dimension
        if f is not None:
            if not isinstance(f, int) or f < 0:
                raise ValueError(f"f must be non-negative integer, got {f}")
            self.f = f

        # Parse metric
        if metric is not None:
            self._metric_type = _parse_metric(metric)  # default METRIC_ANGULAR
            self.metric = _metrictype2str_map.get(self._metric_type, 'angular')
        elif self.f > 0:
            # Lazy default with warning (annoymodule.cc behavior)
            warnings.warn(
                "The default metric will be removed in a future version. "
                "Please specify metric='angular' explicitly.",
                FutureWarning,
                stacklevel=2
            )
            self._metric_type = METRIC_ANGULAR
            self.metric = 'angular'

        # Parse dtype
        self._data_type = _parse_dtype(dtype)
        self.dtype = _datatype2str_map.get(self._data_type, 'float32')

        # Parse index_dtype
        self._index_type = _parse_index_dtype(index_dtype)
        self.index_dtype = _indextype2str_map.get(self._index_type, 'int32')

        # Validate combination
        # if self._metric_type != METRIC_UNKNOWN:
        #     _validate_type_combination(
        #         self._metric_type,
        #         self._index_type,
        #         self._data_type
        #     )

        # Store configuration
        self.n_neighbors = n_neighbors
        self.prefault = prefault
        self.schema_version = schema_version
        self.on_disk_path = on_disk_path

        # Pending configuration
        if seed is not None:
            self._pending_seed = seed
            self._has_pending_seed = True

        if verbose:
            self._pending_verbose = True

        # Create thread lock (like KissGenerator)
        self.lock = threading.RLock()

        # Index NOT created yet (lazy)
        # self._index_created = False

        # Thread safety
        self.lock = threading.RLock()

    # Continue in Part 3...
    # Part 3: Core Methods, Serialization, and Sklearn Compatibility

    # ==========================================================================
    # Magic Methods - Modern sklearn patterns
    # ==========================================================================

    def __repr__(self) -> str:
        """String representation."""
        return f"{self} at 0x{id(self):X}"

    def __str__(self) -> str:
        """Short string representation."""
        return f"{self.__class__.__name__}"

    def __enter__(self):
        """Enter context manager (acquire lock)."""
        self.lock.acquire()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit context manager (release lock)."""
        self.lock.release()
        return False

    def __len__(self) -> int:
        """Return number of items (sklearn compat)."""
        if not self._index_created:
            return 0
        return self.get_n_items()

    # ==========================================================================
    # Internal Helper Methods
    # ==========================================================================

    cdef void _free_index(self) noexcept:
        """Free C++ index."""
        # Free C++ index resources.
        if self._ptr != NULL:
            if self._class != NULL:
                self._class.destructor(self._ptr)

        # Clear all pointers (important for safety)
        self._class = NULL
        self._method_dispatch = NULL
        self._ptr = NULL

        # Clear state flags
        self._index_created = False
        self._index_built = False

    cdef cbool _ensure_index(self) except False:
        """
        Create C++ index if not already created (lazy initialization).

        This is the key pattern from annoymodule.cc ensure_index() (line 1150).
        """
        if self._index_created:
            return True

        if self.f <= 0:
            raise AnnoyDimensionException(
                "Dimension f must be set before creating index"
            )

        if self._metric_type == METRIC_UNKNOWN:
            raise AnnoyMetricException(
                "Metric must be set before creating index"
            )

        # Cache entry factory
        self._class = _lookup_class_factory(
            self._metric_type,
            self._index_type,
            self._data_type
        )
        if self._class == NULL:
            raise ValueError(
                f"Unsupported combination: metric={self._metric_type}, "
                f"index_type={self._index_type}, data_type={self._data_type}"
            )

        # Call factory function (nogil safe)
        self._ptr = self._class.constructor(self.f)
        # Create appropriate C++ index based on type configuration using factory
        # self._ptr = create_index_from_config(
        #     self.f,
        #     self._metric_type,
        #     self._index_type,
        #     self._data_type
        # )
        # Why we cannot use due to Object of type 'void' has no attribute 'unbuild'
        # self._ptr.unbuild(&error)

        if self._ptr == NULL:
            raise RuntimeError("Failed to allocate index")

        # Cache method dispatch
        self._method_dispatch = _lookup_method_factory(
            self._metric_type,
            self._index_type,
            self._data_type
        )

        if self._method_dispatch == NULL:
            raise RuntimeError("Method dispatch lookup failed")

        self._index_created = True

        # Apply pending configuration
        if self._has_pending_seed:
            self.set_seed(self._pending_seed)

        if self._has_pending_verbose and self._pending_verbose:
            self.set_verbose(True)

        return True

    def ensure_index(self):
        """python side."""
        try:
            # Ensure index exists (lazy creation)
            self._ensure_index()
        except Exception as e:
            raise RuntimeError(f"Index not initialized: {e}") from e

    # ==========================================================================
    # Core Annoy Methods - ALL IMPLEMENTED with C-Level Dispatch
    # ==========================================================================
    # -----------------------------------------------------------------------------
    # 1. Index Modification Methods
    # -----------------------------------------------------------------------------
    """
    {{for combo in type_combinations}} if (self._metric_type == {{combo['metric_enum']}} and self._index_type == INDEX_{{combo['index_type'].upper()}} and self._data_type == DATA_{{combo['data_type'].upper()}}):  {{endfor}}
    """

    def add_item(self, item, vector, y=None):
        """
        Add item to index.

        Parameters
        ----------
        item : int
            Item index (non-negative).
        vector : array-like
            Vector of length f.
        y : object, optional
            Label for this item (sklearn compat).

        Returns
        -------
        self : AnnoyIndex
            For method chaining.
        """
        # Infer dimension from first vector if needed
        if self.f == 0:
            self.f = len(vector)

        try:
            # Ensure index exists (lazy creation)
            self._ensure_index()
        except Exception as e:
            raise RuntimeError(f"Index not initialized: {e}") from e

        cdef char* error = NULL
        cdef cbool success = 0

        # Declare ALL possible views up-front not under (if, for, while ...)
        cdef float[:] v_f32
        cdef double[:] v_f64
        cdef uint32_t[:] v_u32
        cdef uint64_t[:] v_u64

        if self._data_type == DATA_FLOAT32:
            v_f32 = vector
            if v_f32.ndim != 1 or v_f32.shape[0] != self.f:
                raise ValueError("vector must be 1D float32 of length f")

            success = self._method_dispatch.add_item(
                self._ptr,
                <int32_t>item,
                <const void*>&v_f32[0],
                &error
            )

        elif self._data_type == DATA_FLOAT64:
            v_f64 = vector
            if v_f64.ndim != 1 or v_f64.shape[0] != self.f:
                raise ValueError("vector must be 1D float64 of length f")

            success = self._method_dispatch.add_item(
                self._ptr,
                <int64_t>item,
                <const void*>&v_f64[0],
                &error
            )

        elif self._data_type == DATA_UINT32:
            v_u32 = vector
            if v_u32.ndim != 1 or v_u32.shape[0] != self.f:
                raise ValueError("vector must be 1D uint32 of length f")

            success = self._method_dispatch.add_item(
                self._ptr,
                <int32_t>item,
                <const void*>&v_u32[0],
                &error
            )

        elif self._data_type == DATA_UINT64:
            v_u64 = vector
            if v_u64.ndim != 1 or v_u64.shape[0] != self.f:
                raise ValueError("vector must be 1D uint64 of length f")

            success = self._method_dispatch.add_item(
                self._ptr,
                <int64_t>item,
                <const void*>&v_u64[0],
                &error
            )

        else:
            raise RuntimeError("Unsupported DataType")

        if not success:
            msg = error.decode("utf-8") if error else "Unknown error"
            raise AnnoyIndexException(f"add_item failed: {msg}")

        # Store label
        if y is not None:
            if self.y_map is None:
                self.y_map = {}
            self.y_map[item] = y

        return self

    def build(self, n_trees=DEFAULT_N_TREES, n_jobs=DEFAULT_N_THREADS):
        """
        Build index forest.

        Parameters
        ----------
        n_trees : int, default=10
            Number of trees to build.
        n_jobs : int, default=-1
            Number of threads (-1 = all cores).

        Returns
        -------
        self : AnnoyIndex
            For method chaining.
        """
        return self

    def unbuild(self):
        """Unload index from memory."""
        cdef char* error = NULL
        # We cannot use due to Object of type 'void' has no attribute 'unbuild'
        # self._ptr.unbuild(&error)
        return self

    def save(self, filename, prefault=None):
        """Save index to disk."""
        if not self._index_built:
            raise AnnoyNotBuiltException("Cannot save index before building")

        prefault = self.prefault if prefault is None else prefault

        cdef bytes filename_bytes = filename.encode('utf-8')
        cdef char* error = NULL
        cdef cbool success

        success = self._method_dispatch.save(
            self._ptr, filename_bytes, prefault, &error
        )

        if not success:
            err_msg = error.decode('utf-8') if error else "Unknown"
            raise IOError(f"save failed: {err_msg}")

        return self

    def load(self, filename, prefault=None):
        """Load index from disk."""
        self._ensure_index()

        prefault = self.prefault if prefault is None else prefault

        cdef bytes filename_bytes = filename.encode('utf-8')
        cdef char* error = NULL
        cdef cbool success

        success = self._method_dispatch.load(
            self._ptr, filename_bytes, prefault, &error
        )

        if not success:
            err_msg = error.decode('utf-8') if error else "Unknown"
            raise IOError(f"load failed: {err_msg}")

        self._index_built = True
        return self

    # def on_disk_build(self, filename):
    #     cdef char* error = NULL
    #     cdef bytes filename_bytes = filename.encode('utf-8')
    #
    #     self._ptr.on_disk_build(filename_bytes, &error)
    #     return self

    # def unload(self):
    #     """Unload index from memory."""
    #     self._ptr.unload()
    #     return self

    def get_n_trees(self) -> int:
        """Return number of trees."""
        if not self._index_created:
            return 0

        cdef int n_trees
        n_trees = self._method_dispatch.get_n_trees(self._ptr)
        return n_trees

    def get_nns_by_item(self, item, n, search_k=DEFAULT_SEARCH_K, include_distances=False):
        """Query nearest neighbors by item index."""
        if not self._index_built:
            raise AnnoyNotBuiltException("Index must be built before querying")

        cdef char* error = NULL

        # Type-aware dispatch (prevent vector type override)
        {{for data in data_types}}
        cdef cpp_vector[{{data['ctype']}}] distances_{{data['name']}}
        {{endfor}}

        {{for index in index_types}}
        cdef cpp_vector[{{index['ctype']}}] result_{{index['name']}}
        {{endfor}}

        # Regular: dispatch based on type
        {{for combo in type_combinations}}
        if (self._metric_type == {{combo['metric_enum']}} and
            self._index_type == INDEX_{{combo['index_type'].upper()}} and
            self._data_type == DATA_{{combo['data_type'].upper()}}):
            self._method_dispatch.get_nns_by_item(
                self._ptr, item, n, search_k, &result_{{combo['index_type']}}, &distances_{{combo['data_type']}}
            )
            neighbors = list(result_{{combo['index_type']}})
            if include_distances:
                return (neighbors, list(distances_{{combo['data_type']}}))
            return neighbors
        {{endfor}}

    def get_nns_by_vector(self, vector, n, search_k=DEFAULT_SEARCH_K, include_distances=False):
        """Query nearest neighbors by vector."""
        if not self._index_built:
            raise AnnoyNotBuiltException("Index must be built before querying")

        cdef char* error = NULL

        # Type-aware dispatch
        {{for data in data_types}}
        cdef {{data['ctype']}}[::1] vec_view_{{data['name']}}
        {{endfor}}

        # Type-aware dispatch (prevent vector type override)
        {{for data in data_types}}
        cdef cpp_vector[{{data['ctype']}}] distances_{{data['name']}}
        {{endfor}}

        {{for index in index_types}}
        cdef cpp_vector[{{index['ctype']}}] result_{{index['name']}}
        {{endfor}}

        # Regular: dispatch based on type
        {{for combo in type_combinations}}
        if (self._metric_type == {{combo['metric_enum']}} and
            self._index_type == INDEX_{{combo['index_type'].upper()}} and
            self._data_type == DATA_{{combo['data_type'].upper()}}):
            vec_np = np.ascontiguousarray(vector, dtype={{combo['data_numpy_type']}})
            vec_view_{{combo['data_type']}} = vec_np
            self._method_dispatch.get_nns_by_vector(
                self._ptr, &vec_view_{{combo['data_type']}}[0], n, search_k, &result_{{combo['index_type']}}, &distances_{{combo['data_type']}}
            )
            neighbors = list(result_{{combo['index_type']}})
            if include_distances:
                return (neighbors, list(distances_{{combo['data_type']}}))
            return neighbors
        {{endfor}}

    def get_item_vector(self, item):
        """Get vector for item."""
        if not self._index_created:
            raise AnnoyIndexException("Index not created")

        # Type-aware dispatch
        {{for data in data_types}}
        cdef {{data['ctype']}}[::1] vec_view_{{data['name']}}
        {{endfor}}

        # Regular: dispatch based on type
        {{for combo in type_combinations}}
        if (self._metric_type == {{combo['metric_enum']}} and
            self._index_type == INDEX_{{combo['index_type'].upper()}} and
            self._data_type == DATA_{{combo['data_type'].upper()}}):
            vec_np = np.empty(self.f, dtype={{combo['data_numpy_type']}})
            vec_view_{{combo['data_type']}} = vec_np
            # with nogil:  # Coercion from Python not allowed without the GIL
            self._method_dispatch.get_item(
                self._ptr, item, &vec_view_{{combo['data_type']}}[0]
            )
            return vec_np
        {{endfor}}

    def get_distance(self, i, j):
        """Get distance between two items."""
        if not self._index_created:
            raise AnnoyIndexException("Index not created")

        cdef double distances
        distances = <double>self._method_dispatch.get_distance(
            self._ptr, i, j
        )
        return distances

    def get_n_items(self) -> int:
        """Return number of items."""
        if not self._index_created:
            return 0

        if self._method_dispatch == NULL:
            raise RuntimeError("Method dispatch not initialized (this should never happen)")

        {{for index in index_types}}
        cdef {{index['ctype']}} n_items_{{index['name']}}
        {{endfor}}

        # Regular: dispatch based on type
        {{for combo in type_combinations}}
        if (self._metric_type == {{combo['metric_enum']}} and
            self._index_type == INDEX_{{combo['index_type'].upper()}} and
            self._data_type == DATA_{{combo['data_type'].upper()}}):
            n_items_{{combo['index_type']}} = self._method_dispatch.get_n_items(
                self._ptr,
            )
            return n_items_{{combo['index_type']}}
        {{endfor}}

    def get_n_trees(self) -> int:
        """Get number of trees."""
        if not self._index_created:
            return 0

        cdef int n_trees
        n_trees = self._method_dispatch.get_n_trees(self._ptr)
        return n_trees

    def get_f(self) -> int:
        """Get dimension."""
        return self.f

    def set_seed(self, seed):
        """Set random seed."""
        if seed < 0:
            raise ValueError("seed must be non-negative")

        if self._index_created and self._ptr != NULL:
            # Apply immediately
            self._method_dispatch.set_seed(self._ptr, <uint64_t>seed)
            return self
        else:
            # Store for later
            self._pending_seed = seed
            self._has_pending_seed = True
        self._seed = seed
        return self

    def set_verbose(self, v):
        """Set verbose mode."""
        if self._index_created and self._ptr != NULL:
            # Apply immediately
            self._ptr.verbose(v)
            return self
        else:
            # Store for later
            self._pending_verbose = v
            self._has_pending_verbose = True
        return self

    # Continue in Part 3 for sklearn methods and serialization...
    # Part 3: Sklearn Compatibility, Serialization, and Module Exports

    # ==========================================================================
    # Sklearn-Style API
    # ==========================================================================

    def get_params(self, deep=True) -> Dict[str, Any]:
        """
        Get estimator parameters (sklearn-style).

        Parameters
        ----------
        deep : bool, default=True
            Included for sklearn API compatibility. Ignored because Annoy
            does not contain nested estimators.

        Returns
        -------
        dict
            Parameter dictionary.
        """
        return {
            'f': self.f,
            'metric': self.metric,
            'n_neighbors': self.n_neighbors,
            'seed': self._pending_seed if self._has_pending_seed else None,
            'verbose': self._pending_verbose if self._has_pending_verbose else False,
            'prefault': self.prefault,
            'schema_version': self.schema_version,
            'on_disk_path': self.on_disk_path,
            'index_dtype': self.index_dtype,
            'dtype': self.dtype,
        }

    def set_params(self, **params):
        """
        Set estimator parameters (sklearn-style).

        Parameters
        ----------
        **params : dict
            Parameters to set.

        Returns
        -------
        self : AnnoyIndex
            For method chaining.

        Notes
        -----
        Changing structural parameters (f, metric, dtype) resets the index.
        """
        for key, value in params.items():
            if key == 'f':
                if self._index_created and self.f != value:
                    warnings.warn(
                        "Changing f resets the index. Call fit() to rebuild.",
                        UserWarning
                    )
                    self._free_index()
                    self._index_created = False
                self.f = value

            elif key == 'metric':
                new_metric_type = _parse_metric(value)
                if self._index_created and self._metric_type != new_metric_type:
                    warnings.warn(
                        "Changing metric resets the index. Call fit() to rebuild.",
                        UserWarning
                    )
                    self._free_index()
                    self._index_created = False
                self._metric_type = new_metric_type
                self.metric = value

            elif key == 'index_dtype':
                new_index_type = _parse_index_dtype(value)
                if self._index_created and self._index_type != new_index_type:
                    warnings.warn(
                        "Changing index_dtype resets the index. Call fit() to rebuild.",
                        UserWarning
                    )
                    self._free_index()
                    self._index_created = False
                self._index_type = new_index_type
                self.index_dtype = _indextype2str_map.get(self._index_type)

            elif key == 'dtype':
                new_data_type = _parse_dtype(value)
                if self._index_created and self._data_type != new_data_type:
                    warnings.warn(
                        "Changing dtype resets the index. Call fit() to rebuild.",
                        UserWarning
                    )
                    self._free_index()
                    self._index_created = False
                self._data_type = new_data_type
                self.dtype = _datatype2str_map.get(self._data_type)

            elif key == 'n_neighbors':
                self.n_neighbors = value

            elif key == 'seed':
                if value is not None:
                    self._pending_seed = value
                    self._has_pending_seed = True
                else:
                    self._has_pending_seed = False

            elif key == 'verbose':
                if value is not None:
                    self._pending_verbose = bool(value)
                    self._has_pending_verbose = True
                else:
                    self._has_pending_verbose = False

            elif key == 'prefault':
                self.prefault = value

            elif key == 'schema_version':
                self.schema_version = value

            elif key == 'on_disk_path':
                self.on_disk_path = value

            else:
                raise ValueError(f"Unknown parameter: {key}")

        return self

    def fit(self, X, y=None, *, n_trees=DEFAULT_N_TREES, n_jobs=DEFAULT_N_THREADS):
        """
        Fit the index (sklearn-style).

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training vectors.
        y : array-like of shape (n_samples,), optional
            Target labels.
        n_trees : int, default=10
            Number of trees to build.
        n_jobs : int, default=-1
            Number of threads.

        Returns
        -------
        self : AnnoyIndex
            Fitted index.
        """
        X = np.asarray(X)

        if X.ndim != 2:
            raise ValueError(f"X must be 2D, got shape {X.shape}")

        n_samples, n_features = X.shape

        # Set dimension if not set
        if self.f == 0:
            self.f = n_features
        elif self.f != n_features:
            raise AnnoyDimensionException(
                f"X has {n_features} features, but index expects {self.f}"
            )

        # Store feature metadata (sklearn SLEP007)
        self.n_features_in_ = n_features

        # Reset and add items
        if self._index_created:
            self._free_index()
            self._index_created = False

        for i in range(n_samples):
            label = y[i] if y is not None else None
            self.add_item(i, X[i], label)

        # Build
        self.build(n_trees, n_jobs)

        return self

    def transform(
        self,
        X,
        *,
        n_neighbors=None,
        search_k=DEFAULT_SEARCH_K,
        include_distances=False,
        return_labels=False
    ):
        """
        Transform queries to neighbor indices (sklearn-style).

        Parameters
        ----------
        X : array-like of shape (n_queries, n_features)
            Query vectors.
        n_neighbors : int or None
            Number of neighbors. If None, uses self.n_neighbors.
        search_k : int, default=-1
            Search parameter.
        include_distances : bool, default=False
            Return distances.
        return_labels : bool, default=False
            Return labels from y_map.

        Returns
        -------
        neighbors : ndarray of shape (n_queries, n_neighbors)
            Neighbor indices.
        distances : ndarray, optional
            If include_distances=True.
        labels : list, optional
            If return_labels=True.
        """
        if not self._index_built:
            raise AnnoyNotBuiltException("fit() must be called before transform()")

        X = np.asarray(X)
        if X.ndim == 1:
            X = X.reshape(1, -1)

        n_queries = X.shape[0]
        k = n_neighbors if n_neighbors is not None else self.n_neighbors

        results = []
        distances_list = [] if include_distances else None
        labels_list = [] if return_labels else None

        for i in range(n_queries):
            if include_distances:
                neighb, dist = self.get_nns_by_vector(
                    X[i], k, search_k, include_distances=True
                )
                results.append(neighb)
                distances_list.append(dist)
            else:
                neighb = self.get_nns_by_vector(X[i], k, search_k)
                results.append(neighb)

            if return_labels and self.y_map:
                labels = [self.y_map.get(idx, None) for idx in neighb]
                labels_list.append(labels)

        results = np.array(results)

        # Return appropriate tuple
        ret = [results]
        if include_distances:
            ret.append(np.array(distances_list))
        if return_labels:
            ret.append(labels_list)

        return tuple(ret) if len(ret) > 1 else ret[0]

    def fit_transform(self, X, y=None, **fit_params):
        """Fit and transform in one call (sklearn-style)."""
        return self.fit(X, y, **fit_params).transform(X)

    @property
    def n_features_in_(self) -> Optional[int]:
        """Number of features seen during fit."""
        return self.f if self.f > 0 else None

    @property
    def n_features_out_(self) -> int:
        """Number of output features (equals n_neighbors)."""
        if not self._index_built:
            raise AttributeError(
                "n_features_out_ is not available before fit()"
            )
        return self.n_neighbors

    def __sklearn_is_fitted__(self) -> bool:
        """Check if fitted (sklearn protocol hook)."""
        return self._index_built

    def __sklearn_clone__(self):
        """Create unfitted clone (sklearn protocol hook)."""
        params = self.get_params()
        return self.__class__(**params)

    # ==========================================================================
    # Serialization Support (following KissGenerator pattern)
    # ==========================================================================

    def __getstate__(self):
        """Return state for pickling."""
        return self.get_state()

    def __setstate__(self, state):
        """Restore state from pickle."""
        self.set_state(state)

    def __reduce__(self):
        """Custom pickle protocol."""
        return (
            self.__class__,
            (),  # No args
            self.__getstate__(),
        )

    def __reduce_ex__(self, protocol):
        """Protocol-specific pickle."""
        return self.__reduce__()

    def get_state(self) -> Dict[str, Any]:
        """
        Get complete state dictionary.

        Returns
        -------
        dict
            State dictionary with all configuration and data.
        """
        state = {
            '__version__': SERIALIZATION_VERSION,
            '__class__': self.__class__.__name__,
            '__module__': self.__module__,

            # Configuration
            'f': self.f,
            'metric': self.metric,
            'dtype': self.dtype,
            'index_dtype': self.index_dtype,
            'n_neighbors': self.n_neighbors,

            # State flags
            'index_created': self._index_created,
            'index_built': self._index_built,

            # Labels
            'y_map': self.y_map,
            'feature_names_in': self.feature_names_in_,

            # Configuration
            'seed': self._pending_seed if self._has_pending_seed else None,
            'verbose': self._pending_verbose if self._has_pending_verbose else None,
            'prefault': self.prefault,
            'schema_version': self.schema_version,
            'on_disk_path': self.on_disk_path,
        }

        # Serialize index data if built
        if self._index_built:
            # Use temporary file
            with tempfile.NamedTemporaryFile(delete=False) as tmp:
                tmp_path = tmp.name

            try:
                self.save(tmp_path)
                with open(tmp_path, 'rb') as f:
                    state['index_data'] = f.read()
            finally:
                if os.path.exists(tmp_path):
                    os.unlink(tmp_path)

        return state

    def set_state(self, state: Dict[str, Any]):
        """
        Restore state from dictionary.

        Parameters
        ----------
        state : dict
            State from get_state().
        """
        # Validate version
        if state.get('__version__') != SERIALIZATION_VERSION:
            warnings.warn(
                f"State version mismatch: {state.get('__version__')} != {SERIALIZATION_VERSION}",
                UserWarning
            )

        # Restore configuration
        self.f = state['f']
        self.metric = state['metric']
        self._metric_type = _parse_metric(self.metric)
        self._data_type = _parse_dtype(state.get('dtype', 'float32'))
        self._index_type = _parse_index_dtype(state.get('index_dtype', 'int32'))

        self.n_neighbors = state['n_neighbors']
        self.y_map = state.get('y_map')
        self.feature_names_in_ = state.get('feature_names_in')

        if state.get('seed') is not None:
            self._pending_seed = state['seed']
            self._has_pending_seed = True

        if state.get('verbose') is not None:
            self._pending_verbose = state['verbose']
            self._has_pending_verbose = True

        self.prefault = state.get('prefault', False)
        self.schema_version = state.get('schema_version', 1)
        self.on_disk_path = state.get('on_disk_path')

        # Restore index data if present
        if 'index_data' in state and state['index_data']:
            with tempfile.NamedTemporaryFile(delete=False) as tmp:
                tmp.write(state['index_data'])
                tmp_path = tmp.name

            try:
                self.load(tmp_path)
            finally:
                if os.path.exists(tmp_path):
                    os.unlink(tmp_path)

    # ==========================================================================
    # Serialization Helpers (JSON)
    # ==========================================================================

    def serialize(self) -> Dict[str, Any]:
        """
        Serialize to JSON-compatible dict.

        Returns
        -------
        dict
            JSON-serializable state.
        """
        state = self.get_state()

        # Encode binary data as base64 for JSON
        if 'index_data' in state and state['index_data']:
            import base64
            state['index_data'] = base64.b64encode(
                state['index_data']
            ).decode('ascii')

        return state

    @classmethod
    def deserialize(cls, data: Dict[str, Any]):
        """
        Deserialize from JSON-compatible dict.

        Parameters
        ----------
        data : dict
            Serialized state.

        Returns
        -------
        AnnoyIndex
            Restored index.
        """
        # Decode base64 if present
        if 'index_data' in data and data['index_data']:
            import base64
            data['index_data'] = base64.b64decode(data['index_data'])

        instance = cls()
        instance.set_state(data)
        return instance

    def to_dict(self) -> Dict[str, Any]:
        """Alias for serialize()."""
        return self.serialize()

    @classmethod
    def from_dict(cls, data: Dict[str, Any]):
        """Alias for deserialize()."""
        return cls.deserialize(data)


# ==============================================================================
# Module-Level Functions
# ==============================================================================

def create_index(f, metric='angular', **kwargs):
    """
    Factory function to create an Annoy index.

    Parameters
    ----------
    f : int
        Vector dimension.
    metric : str, default='angular'
        Distance metric.
    **kwargs
        Additional parameters passed to AnnoyIndex.

    Returns
    -------
    AnnoyIndex
        New index instance.
    """
    return AnnoyIndex(f, metric, **kwargs)

def load_index(filename, f=None, metric=None, **kwargs):
    """
    Load an Annoy index from disk.

    Parameters
    ----------
    filename : str
        Path to index file.
    f : int or None
        Vector dimension (inferred if None).
    metric : str or None
        Distance metric (inferred if None).
    **kwargs
        Additional parameters.

    Returns
    -------
    AnnoyIndex
        Loaded index.
    """
    index = AnnoyIndex(f, metric, **kwargs)
    index.load(filename)
    return index

# ==============================================================================
# Module Initialization
# ==============================================================================

# Module docstring
__doc__ = """
Annoy - Approximate Nearest Neighbors Oh Yeah!

This is a Cython binding to the Annoy C++ library, providing fast approximate
nearest neighbor search with configurable precision and memory usage.

Main Classes:
- AnnoyIndex: Primary user-facing index class

Supported Metrics:
{{for metric in metrics}}
- {{metric['name']}}: {{metric['description']}}
  Aliases: {{', '.join(metric['aliases'])}}
{{endfor}}

Examples:
--------
>>> import numpy as np
>>> from annoylib import AnnoyIndex
>>>
>>> # Create index
>>> index = AnnoyIndex(128, metric='angular')
>>>
>>> # Add vectors
>>> for i in range(1000):
...     vec = np.random.randn(128).astype('float32')
...     index.add_item(i, vec)
>>>
>>> # Build trees
>>> index.build(10)
>>>
>>> # Query
>>> neighbors = index.get_nns_by_item(0, 10)
>>>
>>> # Sklearn-style API
>>> X = np.random.randn(1000, 128)
>>> index = AnnoyIndex(n_neighbors=10)
>>> index.fit(X)
>>> neighbors = index.transform(X[:10])
"""

# Expose main class and functions
__all__ = [
    'AnnoyIndex',
    'create_index',
    'load_index',

    # Exceptions
    'AnnoyException',
    'AnnoyIndexException',
    'AnnoyNotBuiltException',
    'AnnoyDimensionException',
    'AnnoyMetricException',
    'AnnoyMemoryException',
    'AnnoySerializationException',

    # Metadata
    '__version__',
]

# vim: syntax=python ts=4 sw=4 sts=4 et
