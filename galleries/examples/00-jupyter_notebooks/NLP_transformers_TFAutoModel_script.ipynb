{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62360ff0",
   "metadata": {},
   "source": [
    "# This is a Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8bed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvisualkeras: transformers example\\n==========================================\\n\\nAn example showing the :py:func:`~scikitplot.visualkeras` function\\nused by a :py:class:`~tensorflow.keras.Model` or :py:class:`~torch.nn.Module` or\\n:py:class:`~transformers.TFPreTrainedModel` model.\\n\\n.. important::\\n\\n    ‚ö†Ô∏èDeprecated transformers models are not supported to tensorflow use keras-nlp or keras-hub.\\n    `üö´ transformers deprecated models <https://www.linkedin.com/feed/update/urn:li:activity:7338966863403528192/>`_.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "visualkeras: transformers example\n",
    "==========================================\n",
    "\n",
    "An example showing the :py:func:`~scikitplot.visualkeras` function\n",
    "used by a :py:class:`~tensorflow.keras.Model` or :py:class:`~torch.nn.Module` or\n",
    ":py:class:`~transformers.TFPreTrainedModel` model.\n",
    "\n",
    ".. important::\n",
    "\n",
    "    ‚ö†Ô∏è Deprecated Transformers models are not supported in TensorFlow ‚Äî use KerasNLP or KerasHub instead.\n",
    "    `üö´ transformers deprecated models <https://www.linkedin.com/feed/update/urn:li:activity:7338966863403528192/>`_.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe876df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 21:20:39.319164: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-14 21:20:39.339226: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-14 21:20:39.382313: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755206439.428047   92929 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755206439.440232   92929 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755206439.505016   92929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755206439.505152   92929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755206439.505168   92929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755206439.505173   92929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-14 21:20:39.521011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e9a7735b5449b8a7fb074a91abc640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# pip install protobuf==5.29.4\n",
    "import tensorflow as tf\n",
    "\n",
    "# Clear any session to reset the state of TensorFlow/Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "from transformers import TFAutoModel, AutoModel\n",
    "\n",
    "from scikitplot import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7d9405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/micromamba/envs/py311/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.__internal__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py:1086\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1086\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:41\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[32m     32\u001b[39m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     TFTokenClassifierOutput,\n\u001b[32m     40\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     TFCausalLanguageModelingLoss,\n\u001b[32m     43\u001b[39m     TFMaskedLanguageModelingLoss,\n\u001b[32m     44\u001b[39m     TFModelInputType,\n\u001b[32m     45\u001b[39m     TFMultipleChoiceLoss,\n\u001b[32m     46\u001b[39m     TFNextSentencePredictionLoss,\n\u001b[32m     47\u001b[39m     TFPreTrainedModel,\n\u001b[32m     48\u001b[39m     TFQuestionAnsweringLoss,\n\u001b[32m     49\u001b[39m     TFSequenceClassificationLoss,\n\u001b[32m     50\u001b[39m     TFTokenClassificationLoss,\n\u001b[32m     51\u001b[39m     get_initializer,\n\u001b[32m     52\u001b[39m     keras_serializable,\n\u001b[32m     53\u001b[39m     unpack_inputs,\n\u001b[32m     54\u001b[39m )\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtf_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_embeddings_within_bounds, shape_list, stable_softmax\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:76\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_layer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m call_context\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'keras.__internal__'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the Hugging Face transformer model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# transformer_model = AutoModel.from_pretrained(\"microsoft/mpnet-base\")\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# transformer_model = TFAutoModel.from_pretrained(\"microsoft/mpnet-base\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m transformer_model = \u001b[43mTFAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbert-base-uncased\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:483\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    480\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    481\u001b[39m     )\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     model_class = \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    485\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    486\u001b[39m     )\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    488\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    489\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:375\u001b[39m, in \u001b[36m_get_model_class\u001b[39m\u001b[34m(config, model_mapping)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     supported_models = \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    377\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:657\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_mapping:\n\u001b[32m    656\u001b[39m     model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[model_type]\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[32m    660\u001b[39m model_types = [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_mapping.items() \u001b[38;5;28;01mif\u001b[39;00m v == key.\u001b[34m__name__\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:671\u001b[39m, in \u001b[36m_LazyAutoMapping._load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m    670\u001b[39m     \u001b[38;5;28mself\u001b[39m._modules[module_name] = importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformers.models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:616\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, attr):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[32m    618\u001b[39m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py:1076\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1074\u001b[39m     value = \u001b[38;5;28mself\u001b[39m._get_module(name)\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py:1088\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1086\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1089\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1091\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.__internal__'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Hugging Face transformer model\n",
    "# transformer_model = AutoModel.from_pretrained(\"microsoft/mpnet-base\")\n",
    "# transformer_model = TFAutoModel.from_pretrained(\"microsoft/mpnet-base\")\n",
    "transformer_model = TFAutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5077823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a Keras-compatible wrapper for the Hugging Face model\n",
    "def wrap_transformer_model(inputs):\n",
    "    input_ids, attention_mask = inputs\n",
    "    outputs = transformer_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    return outputs.last_hidden_state  # Return the last hidden state for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m attention_mask = tf.keras.Input(shape=(\u001b[32m128\u001b[39m,), dtype=tf.int32, name=\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Pass inputs through the transformer model using a Lambda layer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m seq_len = \u001b[43mtransformer_model\u001b[49m.config.max_position_embeddings  \u001b[38;5;66;03m# usually 512 for BERT, here you used 128\u001b[39;00m\n\u001b[32m      7\u001b[39m hidden_size = transformer_model.config.hidden_size\n\u001b[32m      8\u001b[39m last_hidden_state = tf.keras.layers.Lambda(\n\u001b[32m      9\u001b[39m     wrap_transformer_model,\n\u001b[32m     10\u001b[39m     output_shape=(seq_len, hidden_size),  \u001b[38;5;66;03m# Explicitly specify the output shape\u001b[39;00m\n\u001b[32m     11\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m )([input_ids, attention_mask])\n",
      "\u001b[31mNameError\u001b[39m: name 'transformer_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define Keras model inputs\n",
    "input_ids = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Pass inputs through the transformer model using a Lambda layer\n",
    "seq_len = transformer_model.config.max_position_embeddings  # usually 512 for BERT, here you used 128\n",
    "hidden_size = transformer_model.config.hidden_size\n",
    "last_hidden_state = tf.keras.layers.Lambda(\n",
    "    wrap_transformer_model,\n",
    "    output_shape=(seq_len, hidden_size),  # Explicitly specify the output shape\n",
    "    name=\"bert-base-uncased\",\n",
    ")([input_ids, attention_mask])\n",
    "\n",
    "# Reshape the output to fit into Conv2D (adding extra channel dimension) inside a Lambda layer\n",
    "# def reshape_last_hidden_state(x):\n",
    "#     return tf.reshape(x, (-1, 1, 128, 768))\n",
    "# reshaped_output = tf.keras.layers.Lambda(reshape_last_hidden_state)(last_hidden_state)\n",
    "# Use Reshape layer to reshape the output to fit into Conv2D (adding extra channel dimension)\n",
    "# Reshape to (batch_size, 128, 768, 1) for Conv2D input\n",
    "# reshaped_output = tf.keras.layers.Reshape((seq_len, hidden_size, 1))(last_hidden_state)\n",
    "reshaped_output = tf.keras.layers.Reshape((-1, seq_len, hidden_size))(last_hidden_state)\n",
    "\n",
    "# Add different layers to the model\n",
    "x = tf.keras.layers.Conv2D(\n",
    "    512, (3, 3), activation=\"relu\", padding=\"same\", name=\"conv2d_1\"\n",
    ")(reshaped_output)\n",
    "x = tf.keras.layers.BatchNormalization(name=\"batchnorm_1\")(x)\n",
    "x = tf.keras.layers.Dropout(0.3, name=\"dropout_1\")(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"maxpool_1\")(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(\n",
    "    256, (3, 3), activation=\"relu\", padding=\"same\", name=\"conv2d_2\"\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization(name=\"batchnorm_2\")(x)\n",
    "x = tf.keras.layers.Dropout(0.3, name=\"dropout_2\")(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"maxpool_2\")(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(\n",
    "    128, (3, 3), activation=\"relu\", padding=\"same\", name=\"conv2d_3\"\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization(name=\"batchnorm_3\")(x)\n",
    "x = tf.keras.layers.Dropout(0.4, name=\"dropout_3\")(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"maxpool_3\")(x)\n",
    "\n",
    "# Add GlobalAveragePooling2D before the Dense layers\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"globalaveragepool\")(x)\n",
    "\n",
    "# Add Dense layers\n",
    "x = tf.keras.layers.Dense(512, activation=\"relu\", name=\"dense_1\")(x)\n",
    "x = tf.keras.layers.Dropout(0.5, name=\"dropout_4\")(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"dense_2\")(x)\n",
    "\n",
    "# Add output layer (classification head)\n",
    "dummy_output = tf.keras.layers.Dense(\n",
    "    2, activation=\"softmax\", name=\"dummy_classification_head\"\n",
    ")(x)\n",
    "\n",
    "# Wrap into a Keras model\n",
    "wrapped_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=dummy_output)\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/v3.3.3/keras/src/models/model.py#L217\n",
    "# https://github.com/keras-team/keras/blob/master/keras/src/utils/summary_utils.py#L121\n",
    "wrapped_model.summary(\n",
    "    line_length=None,\n",
    "    positions=None,\n",
    "    print_fn=None,\n",
    "    expand_nested=False,\n",
    "    show_trainable=True,\n",
    "    layer_range=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the wrapped model\n",
    "img_nlp_with_tf_layers = visualkeras.layered_view(\n",
    "    wrapped_model,\n",
    "    legend=True,\n",
    "    show_dimension=True,\n",
    "    min_z=1,\n",
    "    min_xy=1,\n",
    "    max_z=4096,\n",
    "    max_xy=4096,\n",
    "    scale_z=1,\n",
    "    scale_xy=1,\n",
    "    font={\"font_size\": 99},\n",
    "    text_callable=\"default\",\n",
    "    # to_file=\"result_images/nlp_with_tf_layers.png\",\n",
    "    save_fig=True,\n",
    "    save_fig_filename=\"nlp_with_tf_layers.png\",\n",
    "    overwrite=False,\n",
    "    add_timestamp=True,\n",
    "    verbose=True,\n",
    ")\n",
    "img_nlp_with_tf_layers\n",
    "\n",
    "# %%\n",
    "#\n",
    "# .. tags::\n",
    "#\n",
    "#    model-type: classification\n",
    "#    model-workflow: model building\n",
    "#    plot-type: visualkeras\n",
    "#    domain: neural network\n",
    "#    level: advanced\n",
    "#    purpose: showcase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
