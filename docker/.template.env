# This .env file is used to configure environment variables for the application.
# It contains sensitive information such as API keys and service URLs.
# Make sure to keep this file secure and do not share it publicly.

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Shell Parameter Expansion Syntax (for reference in scripts)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# ${VAR}                â†’ Use VAR as-is. If unset or empty, returns an empty string.
#
# ${VAR:?err}           â†’ Exit with 'err' message if VAR is unset or empty.
#                         Useful for enforcing required variables in scripts.
#
# ${VAR:-default}       â†’ Use VAR if set and non-empty; otherwise, use 'default'.
#                         âœ… Best practice for safe defaulting.
#
# ${VAR:=default}       â†’ Use VAR if set and non-empty; otherwise, assign and use 'default'.
#                         Useful for initializing unset or empty variables.
#
# ${VAR-default}        â†’ Use VAR if set (even if empty); otherwise, use 'default'.
#                         âš ï¸ Risky if empty strings are undesirable.
#
# ${VAR+alternative}    â†’ Use 'alternative' if VAR is set (even if empty); otherwise, use nothing.
#                         Useful for conditional logic based on variable presence.

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Application Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# IS_DOCKERENV: Set to true if the application is running inside a Docker container.
# This is used to determine if the app should use Docker-specific configurations.
# Equivelant to IS_DOCKERENV = os.path.exists("/.dockerenv")
IS_DOCKERENV=true  # ğŸ³ Used to detect if the app is running inside Docker


# --------------------------------------------------------------------
# ELASTICSEARCH_URL: Main connection URL used by the app.
# You can override it based on the environment (local, docker internal, or docker to host).
# --------------------------------------------------------------------

# âœ… Used when running the app **locally** on your host machine.
# Example: Elasticsearch is running at http://localhost:9200 on your computer.
ELASTICSEARCH_URL_LOCAL=http://localhost:9200

# âœ… Used when running the app **inside Docker**, and Elasticsearch is another container.
# In Docker Compose, containers communicate using **service names**, not "localhost".
# Example: Elasticsearch service is named "elasticsearch" in your docker-compose.yml
ELASTICSEARCH_URL_DOCKER_INTERNAL=http://elasticsearch:9200

# âœ… Used when the app is in Docker, but Elasticsearch is running **on the host machine**.
# Docker containers can't use "localhost" to refer to the host, so use this:
# "host.docker.internal" is a special DNS name that points to the host (works on Mac/Windows).
# On Linux, add this to docker-compose:
#   extra_hosts:
#     - "host.docker.internal:host-gateway"
ELASTICSEARCH_URL_DOCKER_EXTERNAL=http://host.docker.internal:9200

# ğŸš€ Default value used by the app â€” can be overridden dynamically in code or via Docker Compose
# This example assumes Elasticsearch is running as a Docker service named "elasticsearch".
ELASTICSEARCH_URL=http://elasticsearch:9200


# --------------------------------------------------------------------
# OLLAMA_API_URL: URL of your Ollama inference server (if you're using it).
# This can point to a local or Dockerized Ollama server.
# --------------------------------------------------------------------

# âœ… Used when running the app **locally** (outside Docker).
# Ollama server must be running directly on your machine and accessible at localhost.
# Example: Ollama server running at http://localhost:11434
OLLAMA_API_URL_LOCAL=http://localhost:11434

# âœ… Used when running the app **inside Docker**, and Ollama is running as another service/container.
# In Docker Compose, containers communicate using **service names**, not "localhost".
# Example: Ollama service is named "ollama" in your docker-compose.yml
OLLAMA_API_URL_DOCKER_INTERNAL=http://ollama:11434

# âœ… Used when the app is in Docker, but Ollama is running **on the host machine**.
# "host.docker.internal" lets containers connect back to the host (works on Mac/Windows).
# For Linux, add this to docker-compose:
#   extra_hosts:
#     - "host.docker.internal:host-gateway"
OLLAMA_API_URL_DOCKER_EXTERNAL=http://host.docker.internal:11434

# ğŸš€ Default Ollama API URL used by the app â€” override dynamically if needed.
# This assumes the Ollama server is a service named "ollama" inside Docker.
OLLAMA_API_URL=http://ollama:11434

# --------------------------------------------------------------------

# HUGGINGFACE_API_TOKEN: Required for accessing Hugging Face models and datasets.
# You can obtain it by signing into your Hugging Face account and generating one from your settings.
# âš ï¸ Keep this token secret and out of version control.
HUGGINGFACE_API_TOKEN=

# --------------------------------------------------------------------

# OPENAI_API_KEY: The API key for accessing OpenAI's models (e.g., GPT-4).
# You can get this from https://platform.openai.com/account/api-keys.
# âš ï¸ Keep this key private.
OPENAI_API_KEY=
